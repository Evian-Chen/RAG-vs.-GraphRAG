17:24:43,156 graphrag.config.read_dotenv INFO Loading pipeline .env file
17:24:43,165 graphrag.index.cli INFO using default configuration: {
    "llm": {
        "api_key": "REDACTED, length 164",
        "type": "openai_chat",
        "model": "gpt-4o-mini",
        "max_tokens": 4000,
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "request_timeout": 180.0,
        "api_base": null,
        "api_version": null,
        "proxy": null,
        "cognitive_services_endpoint": null,
        "deployment_name": null,
        "model_supports_json": true,
        "tokens_per_minute": 0,
        "requests_per_minute": 0,
        "max_retries": 10,
        "max_retry_wait": 10.0,
        "sleep_on_rate_limit_recommendation": true,
        "concurrent_requests": 5
    },
    "parallelization": {
        "stagger": 0.2,
        "num_threads": 50
    },
    "async_mode": "threaded",
    "root_dir": "./graphrag_index",
    "reporting": {
        "type": "file",
        "base_dir": "output/${timestamp}/reports",
        "storage_account_blob_url": null
    },
    "storage": {
        "type": "file",
        "base_dir": "output/${timestamp}/artifacts",
        "storage_account_blob_url": null
    },
    "cache": {
        "type": "file",
        "base_dir": "cache",
        "storage_account_blob_url": null
    },
    "input": {
        "type": "file",
        "file_type": "text",
        "base_dir": "input",
        "storage_account_blob_url": null,
        "encoding": "utf-8",
        "file_pattern": ".*\\.txt$",
        "file_filter": null,
        "source_column": null,
        "timestamp_column": null,
        "timestamp_format": null,
        "text_column": "text",
        "title_column": null,
        "document_attribute_columns": []
    },
    "embed_graph": {
        "enabled": false,
        "num_walks": 10,
        "walk_length": 40,
        "window_size": 2,
        "iterations": 3,
        "random_seed": 597832,
        "strategy": null
    },
    "embeddings": {
        "llm": {
            "api_key": "REDACTED, length 164",
            "type": "openai_embedding",
            "model": "text-embedding-3-small",
            "max_tokens": 4000,
            "temperature": 0,
            "top_p": 1,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": null,
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": null,
            "tokens_per_minute": 0,
            "requests_per_minute": 0,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 5
        },
        "parallelization": {
            "stagger": 0.2,
            "num_threads": 50
        },
        "async_mode": "threaded",
        "batch_size": 16,
        "batch_max_tokens": 8191,
        "target": "required",
        "skip": [],
        "vector_store": null,
        "strategy": null
    },
    "chunks": {
        "size": 512,
        "overlap": 64,
        "group_by_columns": [
            "id"
        ],
        "strategy": null
    },
    "snapshots": {
        "graphml": false,
        "raw_entities": false,
        "top_level_nodes": false
    },
    "entity_extraction": {
        "llm": {
            "api_key": "REDACTED, length 164",
            "type": "openai_chat",
            "model": "gpt-4o-mini",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": null,
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 0,
            "requests_per_minute": 0,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 5
        },
        "parallelization": {
            "stagger": 0.2,
            "num_threads": 50
        },
        "async_mode": "threaded",
        "prompt": "prompts/entity_extraction.txt",
        "entity_types": [
            "region",
            "spending_behavior",
            "payer_type",
            "intent",
            "price_bucket",
            "vip_group"
        ],
        "max_gleanings": 12,
        "strategy": null
    },
    "summarize_descriptions": {
        "llm": {
            "api_key": "REDACTED, length 164",
            "type": "openai_chat",
            "model": "gpt-4o-mini",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": null,
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 0,
            "requests_per_minute": 0,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 5
        },
        "parallelization": {
            "stagger": 0.2,
            "num_threads": 50
        },
        "async_mode": "threaded",
        "prompt": "prompts/summarize_descriptions.txt",
        "max_length": 600,
        "strategy": null
    },
    "community_reports": {
        "llm": {
            "api_key": "REDACTED, length 164",
            "type": "openai_chat",
            "model": "gpt-4o-mini",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": null,
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 0,
            "requests_per_minute": 0,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 5
        },
        "parallelization": {
            "stagger": 0.2,
            "num_threads": 50
        },
        "async_mode": "threaded",
        "prompt": "prompts/community_report.txt",
        "max_length": 2000,
        "max_input_length": 8000,
        "strategy": null
    },
    "claim_extraction": {
        "llm": {
            "api_key": "REDACTED, length 164",
            "type": "openai_chat",
            "model": "gpt-4o-mini",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": null,
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 0,
            "requests_per_minute": 0,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 5
        },
        "parallelization": {
            "stagger": 0.2,
            "num_threads": 50
        },
        "async_mode": "threaded",
        "enabled": false,
        "prompt": null,
        "description": "Any claims or facts that could be relevant to information discovery.",
        "max_gleanings": 1,
        "strategy": null
    },
    "cluster_graph": {
        "max_cluster_size": 20,
        "strategy": null
    },
    "umap": {
        "enabled": false
    },
    "local_search": {
        "text_unit_prop": 0.5,
        "community_prop": 0.1,
        "conversation_history_max_turns": 5,
        "top_k_entities": 10,
        "top_k_relationships": 12,
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "max_tokens": 10000,
        "llm_max_tokens": 2000
    },
    "global_search": {
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "max_tokens": 10000,
        "data_max_tokens": 12000,
        "map_max_tokens": 1200,
        "reduce_max_tokens": 2000,
        "concurrency": 32
    },
    "encoding_model": "cl100k_base",
    "skip_workflows": []
}
17:24:43,168 graphrag.index.create_pipeline_config INFO skipping workflows 
17:24:43,186 graphrag.index.run INFO Running pipeline
17:24:43,186 graphrag.index.storage.file_pipeline_storage INFO Creating file storage at graphrag_index/output/20251119-172443/artifacts
17:24:43,186 graphrag.index.input.load_input INFO loading input from root_dir=input
17:24:43,186 graphrag.index.input.load_input INFO using file storage for input
17:24:43,188 graphrag.index.storage.file_pipeline_storage INFO search graphrag_index/input for files matching .*\.txt$
17:24:43,188 graphrag.index.input.text INFO found text files from input, found [('davinci.txt', {})]
17:24:43,191 graphrag.index.input.text INFO Found 1 files, loading 1
17:24:43,195 graphrag.index.workflows.load INFO Workflow Run Order: ['create_base_text_units', 'create_base_extracted_entities', 'create_summarized_entities', 'create_base_entity_graph', 'create_final_entities', 'create_final_nodes', 'create_final_communities', 'join_text_units_to_entity_ids', 'create_final_relationships', 'join_text_units_to_relationship_ids', 'create_final_community_reports', 'create_final_text_units', 'create_base_documents', 'create_final_documents']
17:24:43,195 graphrag.index.run INFO Final # of rows loaded: 1
17:24:43,607 graphrag.index.run INFO Running workflow: create_base_text_units...
17:24:43,607 graphrag.index.run INFO dependencies for create_base_text_units: []
17:24:43,621 datashaper.workflow.workflow INFO executing verb orderby
17:24:43,630 datashaper.workflow.workflow INFO executing verb zip
17:24:43,639 datashaper.workflow.workflow INFO executing verb aggregate_override
17:24:43,654 datashaper.workflow.workflow INFO executing verb chunk
17:24:44,74 datashaper.workflow.workflow INFO executing verb select
17:24:44,81 datashaper.workflow.workflow INFO executing verb unroll
17:24:44,93 datashaper.workflow.workflow INFO executing verb rename
17:24:44,98 datashaper.workflow.workflow INFO executing verb genid
17:24:44,111 datashaper.workflow.workflow INFO executing verb unzip
17:24:44,120 datashaper.workflow.workflow INFO executing verb copy
17:24:44,127 datashaper.workflow.workflow INFO executing verb filter
17:24:44,156 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_base_text_units.parquet
17:24:44,383 graphrag.index.run INFO Running workflow: create_base_extracted_entities...
17:24:44,384 graphrag.index.run INFO dependencies for create_base_extracted_entities: ['create_base_text_units']
17:24:44,384 graphrag.index.run INFO read table from storage: create_base_text_units.parquet
17:24:44,435 datashaper.workflow.workflow INFO executing verb entity_extract
17:24:44,448 graphrag.llm.openai.create_openai_client INFO Creating OpenAI client base_url=None
17:24:44,523 graphrag.index.llm.load_llm INFO create TPM/RPM limiter for gpt-4o-mini: TPM=0, RPM=0
17:24:44,523 graphrag.index.llm.load_llm INFO create concurrency limiter for gpt-4o-mini: 5
17:24:54,161 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
17:24:54,167 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 9.278999999980442. input_tokens=471, output_tokens=399
17:24:54,168 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
17:24:54,169 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
17:24:54,171 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 9.625. input_tokens=471, output_tokens=399
17:24:54,172 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 9.278999999980442. input_tokens=471, output_tokens=396
17:24:54,703 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
17:24:54,709 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 0.5359999999927823. input_tokens=471, output_tokens=5
17:24:54,799 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
17:24:54,800 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 0.6309999999939464. input_tokens=471, output_tokens=5
17:24:55,592 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
17:24:55,593 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 10.701999999990221. input_tokens=471, output_tokens=408
17:24:55,594 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
17:24:55,596 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 10.699999999982538. input_tokens=471, output_tokens=408
17:25:01,780 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
17:25:01,785 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 7.074000000022352. input_tokens=471, output_tokens=399
17:25:04,95 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
17:25:04,99 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 9.296999999991385. input_tokens=471, output_tokens=502
17:25:04,807 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
17:25:04,809 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 0.7090000000025611. input_tokens=471, output_tokens=5
17:25:05,424 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
17:25:05,426 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 0.6160000000090804. input_tokens=471, output_tokens=5
17:25:05,670 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
17:25:05,674 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 11.5. input_tokens=471, output_tokens=631
17:25:07,42 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
17:25:07,47 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 11.448999999993248. input_tokens=471, output_tokens=656
17:25:07,51 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
17:25:07,52 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 11.456999999994878. input_tokens=471, output_tokens=656
17:25:14,27 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
17:25:14,43 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 8.611000000004424. input_tokens=471, output_tokens=399
17:25:14,845 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
17:25:14,848 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 9.171999999991385. input_tokens=471, output_tokens=399
17:25:15,54 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
17:25:15,56 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 13.26500000001397. input_tokens=471, output_tokens=647
17:25:15,464 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
17:25:15,466 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 0.6180000000167638. input_tokens=471, output_tokens=5
17:25:15,536 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
17:25:15,543 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 0.48799999998300336. input_tokens=471, output_tokens=5
17:25:15,867 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
17:25:15,869 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 8.820999999996275. input_tokens=471, output_tokens=399
17:25:15,870 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
17:25:15,872 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 8.818999999988591. input_tokens=471, output_tokens=399
17:25:15,980 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
17:25:15,987 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 0.4429999999993015. input_tokens=471, output_tokens=5
17:25:16,384 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
17:25:16,385 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 0.514999999984866. input_tokens=471, output_tokens=5
17:25:23,551 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
17:25:23,555 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 8.087999999988824. input_tokens=471, output_tokens=399
17:25:24,469 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
17:25:24,477 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 8.605000000010477. input_tokens=471, output_tokens=399
17:25:24,775 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
17:25:24,776 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 8.39000000001397. input_tokens=471, output_tokens=399
17:25:25,84 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
17:25:25,85 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 9.096999999979744. input_tokens=471, output_tokens=395
17:25:25,698 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
17:25:25,699 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 0.614000000001397. input_tokens=471, output_tokens=5
17:25:26,110 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
17:25:26,111 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 12.064000000013039. input_tokens=471, output_tokens=660
17:25:30,678 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
17:25:30,709 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 7.138999999995576. input_tokens=471, output_tokens=399
17:25:31,233 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
17:25:31,234 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 0.51400000002468. input_tokens=471, output_tokens=5
17:25:31,657 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
17:25:31,670 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 7.191000000020722. input_tokens=471, output_tokens=391
17:25:32,957 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
17:25:32,973 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 7.272999999986496. input_tokens=471, output_tokens=399
17:25:33,238 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
17:25:33,244 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 7.13200000001234. input_tokens=471, output_tokens=399
17:25:33,447 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
17:25:33,448 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 0.474000000016531. input_tokens=471, output_tokens=5
17:25:33,708 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
17:25:33,709 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 0.46499999999650754. input_tokens=471, output_tokens=5
17:25:33,971 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
17:25:33,972 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 0.5249999999941792. input_tokens=471, output_tokens=5
17:25:34,504 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
17:25:34,505 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 0.532999999995809. input_tokens=471, output_tokens=5
17:25:35,221 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
17:25:35,222 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 0.7160000000149012. input_tokens=471, output_tokens=5
17:25:35,679 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
17:25:35,680 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 0.4569999999948777. input_tokens=471, output_tokens=5
17:25:36,494 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
17:25:36,495 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 11.71799999999348. input_tokens=471, output_tokens=636
17:25:39,11 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
17:25:39,12 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 7.3399999999965075. input_tokens=471, output_tokens=399
17:25:42,215 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
17:25:42,223 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 10.987999999983003. input_tokens=471, output_tokens=592
17:25:42,860 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
17:25:42,867 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 9.157999999995809. input_tokens=471, output_tokens=399
17:25:44,840 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
17:25:44,842 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 9.162000000011176. input_tokens=471, output_tokens=399
17:25:45,346 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
17:25:45,351 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 0.5090000000200234. input_tokens=471, output_tokens=5
17:25:46,908 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
17:25:46,909 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 10.413999999989755. input_tokens=471, output_tokens=399
17:25:48,25 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
17:25:48,26 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 1.1169999999983702. input_tokens=19, output_tokens=21
17:25:49,459 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
17:25:49,472 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 10.455000000016298. input_tokens=471, output_tokens=399
17:25:50,347 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
17:25:50,352 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 0.875. input_tokens=19, output_tokens=19
17:25:51,654 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
17:25:51,657 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 1.3040000000037253. input_tokens=19, output_tokens=19
17:25:54,374 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
17:25:54,376 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 12.14999999999418. input_tokens=471, output_tokens=399
17:25:54,506 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
17:25:54,513 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 11.645000000018626. input_tokens=471, output_tokens=490
17:25:55,496 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
17:25:55,498 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 0.9839999999967404. input_tokens=19, output_tokens=20
17:25:58,879 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
17:25:58,881 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 13.528999999980442. input_tokens=19, output_tokens=508
17:25:59,900 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
17:25:59,901 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 1.0199999999895226. input_tokens=19, output_tokens=19
17:26:00,936 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
17:26:00,940 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 1.0389999999897555. input_tokens=19, output_tokens=19
17:26:03,911 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
17:26:03,922 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 12.25999999998021. input_tokens=19, output_tokens=584
17:26:06,350 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
17:26:06,353 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 11.97599999999511. input_tokens=19, output_tokens=582
17:26:12,493 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
17:26:12,495 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
17:26:12,502 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 11.558999999979278. input_tokens=19, output_tokens=447
17:26:12,506 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 24.476000000024214. input_tokens=19, output_tokens=991
17:26:14,235 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
17:26:14,238 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 1.7190000000118744. input_tokens=19, output_tokens=20
17:26:21,829 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
17:26:21,837 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 15.481999999989057. input_tokens=19, output_tokens=523
17:26:22,803 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
17:26:22,814 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 0.9750000000058208. input_tokens=19, output_tokens=19
17:26:23,158 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
17:26:23,171 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 19.245000000024447. input_tokens=19, output_tokens=679
17:26:23,853 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
17:26:23,859 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 1.0450000000128057. input_tokens=19, output_tokens=19
17:26:24,149 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
17:26:24,157 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 0.9860000000044238. input_tokens=19, output_tokens=21
17:26:24,679 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
17:26:24,683 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 0.8239999999932479. input_tokens=19, output_tokens=20
17:26:25,515 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
17:26:25,517 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 1.3589999999967404. input_tokens=19, output_tokens=19
17:26:26,521 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
17:26:26,522 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 1.838999999978114. input_tokens=19, output_tokens=19
17:26:31,856 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
17:26:31,889 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 17.638999999995576. input_tokens=19, output_tokens=924
17:26:33,187 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
17:26:33,190 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 20.68299999998999. input_tokens=19, output_tokens=653
17:26:34,33 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
17:26:34,40 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 8.521999999997206. input_tokens=19, output_tokens=509
17:26:35,24 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
17:26:35,26 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 0.985000000015134. input_tokens=19, output_tokens=19
17:26:39,739 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
17:26:39,745 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 13.220000000001164. input_tokens=19, output_tokens=597
17:26:40,794 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
17:26:40,795 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 1.0499999999883585. input_tokens=19, output_tokens=20
17:26:42,167 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
17:26:42,168 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 1.3729999999923166. input_tokens=19, output_tokens=19
17:26:44,852 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
17:26:44,858 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 12.964999999996508. input_tokens=19, output_tokens=602
17:26:45,877 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
17:26:45,878 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 1.0190000000002328. input_tokens=19, output_tokens=21
17:26:46,965 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
17:26:46,974 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 13.781999999977415. input_tokens=19, output_tokens=593
17:26:48,97 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
17:26:48,134 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 1.1589999999850988. input_tokens=19, output_tokens=19
17:26:48,168 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
17:26:48,170 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 13.142000000021653. input_tokens=19, output_tokens=663
17:26:49,361 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
17:26:49,362 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 1.228000000002794. input_tokens=19, output_tokens=19
17:26:49,740 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
17:26:49,745 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 1.573999999993248. input_tokens=19, output_tokens=19
17:26:50,801 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
17:26:50,802 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 1.0570000000006985. input_tokens=19, output_tokens=19
17:26:50,848 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
17:26:50,850 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 1.4879999999830034. input_tokens=19, output_tokens=19
17:26:52,168 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
17:26:52,169 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 1.3669999999983702. input_tokens=19, output_tokens=19
17:26:53,355 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
17:26:53,358 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 1.187000000005355. input_tokens=19, output_tokens=21
17:26:54,919 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
17:26:54,924 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 12.754000000015367. input_tokens=19, output_tokens=545
17:26:59,706 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
17:26:59,708 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 13.828999999997905. input_tokens=19, output_tokens=506
17:27:04,418 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
17:27:04,420 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 11.061000000016065. input_tokens=19, output_tokens=481
17:27:04,824 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
17:27:04,826 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 13.97500000000582. input_tokens=19, output_tokens=773
17:27:05,537 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
17:27:05,539 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 1.1190000000060536. input_tokens=19, output_tokens=19
17:27:05,951 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
17:27:05,953 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-loopcheck-0" with 0 retries took 0.41399999998975545. input_tokens=26, output_tokens=2
17:27:06,456 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
17:27:06,457 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-loopcheck-0" with 0 retries took 0.503000000026077. input_tokens=26, output_tokens=2
17:27:07,75 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
17:27:07,78 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-loopcheck-0" with 0 retries took 0.6209999999846332. input_tokens=26, output_tokens=2
17:27:07,600 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
17:27:07,601 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-loopcheck-0" with 0 retries took 0.5230000000155997. input_tokens=26, output_tokens=2
17:27:08,103 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
17:27:08,105 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-loopcheck-0" with 0 retries took 0.503999999986263. input_tokens=26, output_tokens=2
17:27:08,651 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
17:27:08,653 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-loopcheck-0" with 0 retries took 0.5480000000097789. input_tokens=26, output_tokens=2
17:27:09,38 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
17:27:09,43 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-loopcheck-0" with 0 retries took 0.3889999999955762. input_tokens=26, output_tokens=2
17:27:09,432 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
17:27:09,433 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-loopcheck-0" with 0 retries took 0.39000000001396984. input_tokens=26, output_tokens=2
17:27:09,626 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
17:27:09,627 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 14.703000000008615. input_tokens=19, output_tokens=626
17:27:10,150 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
17:27:10,152 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-loopcheck-0" with 0 retries took 0.7189999999827705. input_tokens=26, output_tokens=2
17:27:10,534 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
17:27:10,543 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-loopcheck-0" with 0 retries took 0.9149999999790452. input_tokens=26, output_tokens=2
17:27:10,583 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
17:27:10,610 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-loopcheck-0" with 0 retries took 0.45700000002398156. input_tokens=26, output_tokens=2
17:27:10,932 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
17:27:10,938 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-loopcheck-0" with 0 retries took 0.39500000001862645. input_tokens=26, output_tokens=2
17:27:11,59 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
17:27:11,72 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-loopcheck-0" with 0 retries took 0.46199999999953434. input_tokens=26, output_tokens=2
17:27:11,470 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
17:27:11,471 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-loopcheck-0" with 0 retries took 0.39900000000488944. input_tokens=26, output_tokens=2
17:27:11,581 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
17:27:11,582 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-loopcheck-0" with 0 retries took 0.6440000000002328. input_tokens=26, output_tokens=2
17:27:11,855 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
17:27:11,860 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-loopcheck-0" with 0 retries took 0.3880000000062864. input_tokens=26, output_tokens=2
17:27:12,51 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
17:27:12,58 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-loopcheck-0" with 0 retries took 0.47500000000582077. input_tokens=26, output_tokens=2
17:27:12,264 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
17:27:12,267 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-loopcheck-0" with 0 retries took 0.40700000000651926. input_tokens=26, output_tokens=2
17:27:12,584 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
17:27:12,587 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-loopcheck-0" with 0 retries took 0.5289999999804422. input_tokens=26, output_tokens=2
17:27:12,653 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
17:27:12,658 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-loopcheck-0" with 0 retries took 0.39100000000325963. input_tokens=26, output_tokens=2
17:27:12,927 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
17:27:12,929 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-loopcheck-0" with 0 retries took 0.34200000000419095. input_tokens=26, output_tokens=2
17:27:13,277 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
17:27:13,294 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-loopcheck-0" with 0 retries took 0.635999999998603. input_tokens=26, output_tokens=2
17:27:13,431 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
17:27:13,433 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 13.723999999987427. input_tokens=19, output_tokens=650
17:27:13,834 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
17:27:13,836 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-loopcheck-0" with 0 retries took 0.40200000000186265. input_tokens=26, output_tokens=2
17:27:13,924 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
17:27:13,928 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-loopcheck-0" with 0 retries took 0.6330000000016298. input_tokens=26, output_tokens=2
17:27:14,244 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
17:27:14,245 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-loopcheck-0" with 0 retries took 0.40900000001420267. input_tokens=26, output_tokens=2
17:27:14,417 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
17:27:14,421 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-loopcheck-0" with 0 retries took 0.49299999998766. input_tokens=26, output_tokens=2
17:27:14,620 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
17:27:14,638 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-loopcheck-0" with 0 retries took 0.3919999999925494. input_tokens=26, output_tokens=2
17:27:15,169 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
17:27:15,171 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-loopcheck-0" with 0 retries took 2.24199999999837. input_tokens=26, output_tokens=2
17:27:15,172 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
17:27:15,175 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-loopcheck-0" with 0 retries took 0.5370000000111759. input_tokens=26, output_tokens=2
17:27:15,760 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
17:27:15,766 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-loopcheck-0" with 0 retries took 0.5939999999827705. input_tokens=26, output_tokens=2
17:27:16,383 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
17:27:16,385 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-loopcheck-0" with 0 retries took 1.2090000000025611. input_tokens=26, output_tokens=2
17:27:16,386 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
17:27:16,390 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-loopcheck-0" with 0 retries took 1.9690000000118744. input_tokens=26, output_tokens=2
17:27:16,669 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
17:27:16,670 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-loopcheck-0" with 0 retries took 0.9040000000095461. input_tokens=26, output_tokens=2
17:27:16,750 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
17:27:16,758 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-loopcheck-0" with 0 retries took 0.3709999999846332. input_tokens=26, output_tokens=2
17:27:16,916 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
17:27:16,923 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-loopcheck-0" with 0 retries took 0.532999999995809. input_tokens=26, output_tokens=2
17:27:17,23 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
17:27:17,29 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 12.203000000008615. input_tokens=19, output_tokens=601
17:27:17,249 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
17:27:17,261 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-loopcheck-0" with 0 retries took 0.5019999999785796. input_tokens=26, output_tokens=2
17:27:17,285 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
17:27:17,287 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-loopcheck-0" with 0 retries took 0.364000000001397. input_tokens=26, output_tokens=2
17:27:17,322 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
17:27:17,324 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-loopcheck-0" with 0 retries took 0.6540000000095461. input_tokens=26, output_tokens=2
17:27:17,623 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
17:27:17,630 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-loopcheck-0" with 0 retries took 0.34200000000419095. input_tokens=26, output_tokens=2
17:27:17,832 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
17:27:17,835 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-loopcheck-0" with 0 retries took 0.510999999998603. input_tokens=26, output_tokens=2
17:27:17,933 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
17:27:17,937 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-loopcheck-0" with 0 retries took 0.6760000000067521. input_tokens=26, output_tokens=2
17:27:18,44 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
17:27:18,46 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-loopcheck-0" with 0 retries took 0.41599999999743886. input_tokens=26, output_tokens=1
17:27:18,114 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
17:27:18,122 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-loopcheck-0" with 0 retries took 1.092000000004191. input_tokens=26, output_tokens=2
17:27:18,312 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
17:27:18,317 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-loopcheck-0" with 0 retries took 0.3779999999969732. input_tokens=26, output_tokens=1
17:27:18,337 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
17:27:18,345 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-loopcheck-0" with 0 retries took 0.5089999999909196. input_tokens=26, output_tokens=2
17:27:18,545 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
17:27:18,546 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-loopcheck-0" with 0 retries took 0.4989999999816064. input_tokens=26, output_tokens=2
17:27:42,822 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
17:27:42,834 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 107.33099999997648. input_tokens=19, output_tokens=3991
17:27:42,860 datashaper.workflow.workflow INFO executing verb merge_graphs
17:27:42,993 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_base_extracted_entities.parquet
17:27:43,205 graphrag.index.run INFO Running workflow: create_summarized_entities...
17:27:43,205 graphrag.index.run INFO dependencies for create_summarized_entities: ['create_base_extracted_entities']
17:27:43,205 graphrag.index.run INFO read table from storage: create_base_extracted_entities.parquet
17:27:43,228 datashaper.workflow.workflow INFO executing verb summarize_descriptions
17:27:45,841 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
17:27:45,849 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.5859999999811407. input_tokens=203, output_tokens=100
17:27:45,954 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
17:27:45,955 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.690999999991618. input_tokens=203, output_tokens=99
17:27:46,146 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
17:27:46,151 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.885999999998603. input_tokens=203, output_tokens=96
17:27:46,416 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
17:27:46,418 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 3.1520000000018626. input_tokens=203, output_tokens=86
17:27:46,593 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
17:27:46,595 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 3.327999999979511. input_tokens=203, output_tokens=101
17:27:48,856 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
17:27:48,860 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.4400000000023283. input_tokens=203, output_tokens=90
17:27:48,912 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
17:27:48,914 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.7620000000169966. input_tokens=203, output_tokens=103
17:27:49,127 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
17:27:49,136 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 3.180999999982305. input_tokens=203, output_tokens=102
17:27:49,476 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
17:27:49,477 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 3.627999999996973. input_tokens=203, output_tokens=99
17:27:50,602 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
17:27:50,605 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 4.00899999999092. input_tokens=203, output_tokens=100
17:27:51,376 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
17:27:51,378 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.518000000010943. input_tokens=203, output_tokens=102
17:27:51,900 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
17:27:51,903 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.7660000000032596. input_tokens=203, output_tokens=105
17:27:52,540 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
17:27:52,542 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 3.0640000000130385. input_tokens=203, output_tokens=102
17:27:52,783 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
17:27:52,812 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 3.896999999997206. input_tokens=203, output_tokens=95
17:27:53,266 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
17:27:53,270 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.6630000000004657. input_tokens=203, output_tokens=90
17:27:54,84 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
17:27:54,91 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.7119999999995343. input_tokens=203, output_tokens=90
17:27:54,235 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
17:27:54,237 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.3330000000132713. input_tokens=203, output_tokens=80
17:27:54,970 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
17:27:54,973 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.1609999999927823. input_tokens=203, output_tokens=86
17:27:56,597 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
17:27:56,599 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 4.0570000000006985. input_tokens=203, output_tokens=106
17:27:56,708 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
17:27:56,721 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.629000000015367. input_tokens=203, output_tokens=98
17:27:56,957 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
17:27:56,958 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
17:27:56,960 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.7220000000088476. input_tokens=203, output_tokens=102
17:27:56,961 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 3.690999999991618. input_tokens=203, output_tokens=102
17:27:57,979 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
17:27:57,991 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 3.0169999999925494. input_tokens=203, output_tokens=102
17:27:59,155 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
17:27:59,157 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.1959999999962747. input_tokens=203, output_tokens=80
17:27:59,218 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
17:27:59,220 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.6199999999953434. input_tokens=203, output_tokens=102
17:27:59,634 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
17:27:59,636 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.9149999999790452. input_tokens=203, output_tokens=103
17:28:00,49 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
17:28:00,58 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 3.0960000000195578. input_tokens=203, output_tokens=90
17:28:01,457 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
17:28:01,460 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 3.4680000000225846. input_tokens=203, output_tokens=102
17:28:01,888 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
17:28:01,892 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.6710000000020955. input_tokens=203, output_tokens=90
17:28:02,375 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
17:28:02,377 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.7410000000090804. input_tokens=203, output_tokens=103
17:28:02,461 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
17:28:02,468 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 3.308999999979278. input_tokens=203, output_tokens=84
17:28:03,607 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
17:28:03,608 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 3.5489999999990687. input_tokens=203, output_tokens=90
17:28:04,521 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
17:28:04,523 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 3.0609999999869615. input_tokens=203, output_tokens=105
17:28:04,568 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
17:28:04,575 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.6820000000006985. input_tokens=203, output_tokens=95
17:28:05,443 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
17:28:05,447 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.978000000002794. input_tokens=203, output_tokens=105
17:28:06,72 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
17:28:06,76 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 3.6979999999748543. input_tokens=203, output_tokens=95
17:28:06,784 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
17:28:06,787 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 3.1779999999853317. input_tokens=203, output_tokens=102
17:28:07,315 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
17:28:07,317 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.74199999999837. input_tokens=203, output_tokens=85
17:28:07,399 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
17:28:07,401 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.877999999996973. input_tokens=203, output_tokens=105
17:28:08,559 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
17:28:08,562 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 3.114000000001397. input_tokens=203, output_tokens=102
17:28:09,745 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
17:28:09,749 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 3.6710000000020955. input_tokens=203, output_tokens=102
17:28:09,750 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
17:28:09,754 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.966000000014901. input_tokens=203, output_tokens=85
17:28:10,320 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
17:28:10,324 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.9210000000020955. input_tokens=203, output_tokens=100
17:28:10,511 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
17:28:10,529 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 3.2110000000102445. input_tokens=203, output_tokens=99
17:28:11,470 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
17:28:11,472 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.907999999995809. input_tokens=203, output_tokens=99
17:28:12,11 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
17:28:12,13 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.261999999987893. input_tokens=203, output_tokens=80
17:28:12,476 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
17:28:12,494 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.739000000001397. input_tokens=203, output_tokens=103
17:28:12,817 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
17:28:12,819 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.2890000000188593. input_tokens=203, output_tokens=90
17:28:14,367 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
17:28:14,368 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.8960000000079162. input_tokens=203, output_tokens=90
17:28:14,613 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
17:28:14,626 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 4.2999999999883585. input_tokens=203, output_tokens=100
17:28:14,650 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_summarized_entities.parquet
17:28:14,938 graphrag.index.run INFO Running workflow: create_base_entity_graph...
17:28:14,938 graphrag.index.run INFO dependencies for create_base_entity_graph: ['create_summarized_entities']
17:28:14,938 graphrag.index.run INFO read table from storage: create_summarized_entities.parquet
17:28:14,953 datashaper.workflow.workflow INFO executing verb cluster_graph
17:28:14,993 datashaper.workflow.workflow INFO executing verb select
17:28:14,995 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_base_entity_graph.parquet
17:28:15,123 graphrag.index.run INFO Running workflow: create_final_entities...
17:28:15,124 graphrag.index.run INFO dependencies for create_final_entities: ['create_base_entity_graph']
17:28:15,124 graphrag.index.run INFO read table from storage: create_base_entity_graph.parquet
17:28:15,142 datashaper.workflow.workflow INFO executing verb unpack_graph
17:28:15,157 datashaper.workflow.workflow INFO executing verb rename
17:28:15,166 datashaper.workflow.workflow INFO executing verb select
17:28:15,176 datashaper.workflow.workflow INFO executing verb dedupe
17:28:15,188 datashaper.workflow.workflow INFO executing verb rename
17:28:15,197 datashaper.workflow.workflow INFO executing verb filter
17:28:15,223 datashaper.workflow.workflow INFO executing verb text_split
17:28:15,239 datashaper.workflow.workflow INFO executing verb drop
17:28:15,264 datashaper.workflow.workflow INFO executing verb merge
17:28:15,323 datashaper.workflow.workflow INFO executing verb text_embed
17:28:15,326 graphrag.llm.openai.create_openai_client INFO Creating OpenAI client base_url=None
17:28:15,349 graphrag.index.llm.load_llm INFO create TPM/RPM limiter for text-embedding-3-small: TPM=0, RPM=0
17:28:15,349 graphrag.index.llm.load_llm INFO create concurrency limiter for text-embedding-3-small: 5
17:28:15,378 graphrag.index.verbs.text.embed.strategies.openai INFO embedding 170 inputs via 170 snippets using 11 batches. max_batch_size=16, max_tokens=8191
17:28:16,97 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
17:28:16,104 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
17:28:16,112 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.7229999999981374. input_tokens=256, output_tokens=0
17:28:16,222 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
17:28:16,329 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.9449999999778811. input_tokens=1640, output_tokens=0
17:28:16,350 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
17:28:16,379 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.9880000000121072. input_tokens=322, output_tokens=0
17:28:16,441 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.0530000000144355. input_tokens=1431, output_tokens=0
17:28:16,588 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
17:28:16,697 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
17:28:16,735 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.342000000004191. input_tokens=1292, output_tokens=0
17:28:16,950 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
17:28:16,970 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
17:28:16,982 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.8350000000209548. input_tokens=301, output_tokens=0
17:28:17,163 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.8119999999762513. input_tokens=337, output_tokens=0
17:28:17,182 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
17:28:17,209 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.8089999999792781. input_tokens=323, output_tokens=0
17:28:17,227 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
17:28:17,342 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.8830000000016298. input_tokens=444, output_tokens=0
17:28:17,363 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.606999999989057. input_tokens=512, output_tokens=0
17:28:18,141 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
17:28:18,146 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.143999999971129. input_tokens=205, output_tokens=0
17:28:18,185 datashaper.workflow.workflow INFO executing verb drop
17:28:18,194 datashaper.workflow.workflow INFO executing verb filter
17:28:18,208 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_entities.parquet
17:28:18,407 graphrag.index.run INFO Running workflow: create_final_nodes...
17:28:18,407 graphrag.index.run INFO dependencies for create_final_nodes: ['create_base_entity_graph']
17:28:18,407 graphrag.index.run INFO read table from storage: create_base_entity_graph.parquet
17:28:18,426 datashaper.workflow.workflow INFO executing verb layout_graph
17:28:18,478 datashaper.workflow.workflow INFO executing verb unpack_graph
17:28:18,591 datashaper.workflow.workflow INFO executing verb unpack_graph
17:28:18,608 datashaper.workflow.workflow INFO executing verb filter
17:28:18,631 datashaper.workflow.workflow INFO executing verb drop
17:28:18,643 datashaper.workflow.workflow INFO executing verb select
17:28:18,654 datashaper.workflow.workflow INFO executing verb rename
17:28:18,666 datashaper.workflow.workflow INFO executing verb join
17:28:18,687 datashaper.workflow.workflow INFO executing verb convert
17:28:18,733 datashaper.workflow.workflow INFO executing verb rename
17:28:18,735 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_nodes.parquet
17:28:18,901 graphrag.index.run INFO Running workflow: create_final_communities...
17:28:18,902 graphrag.index.run INFO dependencies for create_final_communities: ['create_base_entity_graph']
17:28:18,902 graphrag.index.run INFO read table from storage: create_base_entity_graph.parquet
17:28:18,928 datashaper.workflow.workflow INFO executing verb unpack_graph
17:28:18,949 datashaper.workflow.workflow INFO executing verb unpack_graph
17:28:18,969 datashaper.workflow.workflow INFO executing verb aggregate_override
17:28:18,987 datashaper.workflow.workflow INFO executing verb join
17:28:19,3 datashaper.workflow.workflow INFO executing verb join
17:28:19,20 datashaper.workflow.workflow INFO executing verb concat
17:28:19,32 datashaper.workflow.workflow INFO executing verb filter
17:28:19,60 datashaper.workflow.workflow INFO executing verb aggregate_override
17:28:19,76 datashaper.workflow.workflow INFO executing verb join
17:28:19,93 datashaper.workflow.workflow INFO executing verb filter
17:28:19,123 datashaper.workflow.workflow INFO executing verb fill
17:28:19,136 datashaper.workflow.workflow INFO executing verb merge
17:28:19,151 datashaper.workflow.workflow INFO executing verb copy
17:28:19,165 datashaper.workflow.workflow INFO executing verb select
17:28:19,167 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_communities.parquet
17:28:19,311 graphrag.index.run INFO Running workflow: join_text_units_to_entity_ids...
17:28:19,311 graphrag.index.run INFO dependencies for join_text_units_to_entity_ids: ['create_final_entities']
17:28:19,312 graphrag.index.run INFO read table from storage: create_final_entities.parquet
17:28:19,350 datashaper.workflow.workflow INFO executing verb select
17:28:19,364 datashaper.workflow.workflow INFO executing verb unroll
17:28:19,380 datashaper.workflow.workflow INFO executing verb aggregate_override
17:28:19,388 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table join_text_units_to_entity_ids.parquet
17:28:19,538 graphrag.index.run INFO Running workflow: create_final_relationships...
17:28:19,539 graphrag.index.run INFO dependencies for create_final_relationships: ['create_base_entity_graph', 'create_final_nodes']
17:28:19,539 graphrag.index.run INFO read table from storage: create_base_entity_graph.parquet
17:28:19,542 graphrag.index.run INFO read table from storage: create_final_nodes.parquet
17:28:19,594 datashaper.workflow.workflow INFO executing verb unpack_graph
17:28:19,616 datashaper.workflow.workflow INFO executing verb filter
17:28:19,650 datashaper.workflow.workflow INFO executing verb rename
17:28:19,666 datashaper.workflow.workflow INFO executing verb filter
17:28:19,701 datashaper.workflow.workflow INFO executing verb drop
17:28:19,718 datashaper.workflow.workflow INFO executing verb compute_edge_combined_degree
17:28:19,741 datashaper.workflow.workflow INFO executing verb convert
17:28:19,779 datashaper.workflow.workflow INFO executing verb convert
17:28:19,780 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_relationships.parquet
17:28:19,966 graphrag.index.run INFO Running workflow: join_text_units_to_relationship_ids...
17:28:19,967 graphrag.index.run INFO dependencies for join_text_units_to_relationship_ids: ['create_final_relationships']
17:28:19,967 graphrag.index.run INFO read table from storage: create_final_relationships.parquet
17:28:20,7 datashaper.workflow.workflow INFO executing verb select
17:28:20,27 datashaper.workflow.workflow INFO executing verb unroll
17:28:20,47 datashaper.workflow.workflow INFO executing verb aggregate_override
17:28:20,72 datashaper.workflow.workflow INFO executing verb select
17:28:20,75 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table join_text_units_to_relationship_ids.parquet
17:28:20,224 graphrag.index.run INFO Running workflow: create_final_community_reports...
17:28:20,230 graphrag.index.run INFO dependencies for create_final_community_reports: ['create_final_relationships', 'create_final_nodes']
17:28:20,234 graphrag.index.run INFO read table from storage: create_final_relationships.parquet
17:28:20,238 graphrag.index.run INFO read table from storage: create_final_nodes.parquet
17:28:20,276 datashaper.workflow.workflow INFO executing verb prepare_community_reports_nodes
17:28:20,296 datashaper.workflow.workflow INFO executing verb prepare_community_reports_edges
17:28:20,315 datashaper.workflow.workflow INFO executing verb restore_community_hierarchy
17:28:20,337 datashaper.workflow.workflow INFO executing verb prepare_community_reports
17:28:20,337 graphrag.index.verbs.graph.report.prepare_community_reports INFO Number of nodes at level=0 => 170
17:28:20,381 datashaper.workflow.workflow INFO executing verb create_community_reports
17:28:23,578 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
17:28:23,579 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 3.190999999991618. input_tokens=385, output_tokens=123
17:28:23,629 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
17:28:23,630 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 3.2379999999830034. input_tokens=385, output_tokens=133
17:28:23,733 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
17:28:23,760 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 3.3690000000060536. input_tokens=385, output_tokens=136
17:28:24,613 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
17:28:24,615 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 4.225000000005821. input_tokens=385, output_tokens=155
17:28:24,669 datashaper.workflow.workflow INFO executing verb window
17:28:24,671 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_community_reports.parquet
17:28:24,820 graphrag.index.run INFO Running workflow: create_final_text_units...
17:28:24,820 graphrag.index.run INFO dependencies for create_final_text_units: ['join_text_units_to_relationship_ids', 'join_text_units_to_entity_ids', 'create_base_text_units']
17:28:24,820 graphrag.index.run INFO read table from storage: join_text_units_to_relationship_ids.parquet
17:28:24,823 graphrag.index.run INFO read table from storage: join_text_units_to_entity_ids.parquet
17:28:24,825 graphrag.index.run INFO read table from storage: create_base_text_units.parquet
17:28:24,865 datashaper.workflow.workflow INFO executing verb select
17:28:24,885 datashaper.workflow.workflow INFO executing verb rename
17:28:24,908 datashaper.workflow.workflow INFO executing verb join
17:28:24,935 datashaper.workflow.workflow INFO executing verb join
17:28:24,960 datashaper.workflow.workflow INFO executing verb aggregate_override
17:28:24,984 datashaper.workflow.workflow INFO executing verb select
17:28:24,986 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_text_units.parquet
17:28:25,133 graphrag.index.run INFO Running workflow: create_base_documents...
17:28:25,133 graphrag.index.run INFO dependencies for create_base_documents: ['create_final_text_units']
17:28:25,133 graphrag.index.run INFO read table from storage: create_final_text_units.parquet
17:28:25,176 datashaper.workflow.workflow INFO executing verb unroll
17:28:25,199 datashaper.workflow.workflow INFO executing verb select
17:28:25,220 datashaper.workflow.workflow INFO executing verb rename
17:28:25,242 datashaper.workflow.workflow INFO executing verb join
17:28:25,266 datashaper.workflow.workflow INFO executing verb aggregate_override
17:28:25,289 datashaper.workflow.workflow INFO executing verb join
17:28:25,315 datashaper.workflow.workflow INFO executing verb rename
17:28:25,337 datashaper.workflow.workflow INFO executing verb convert
17:28:25,386 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_base_documents.parquet
17:28:25,533 graphrag.index.run INFO Running workflow: create_final_documents...
17:28:25,533 graphrag.index.run INFO dependencies for create_final_documents: ['create_base_documents']
17:28:25,534 graphrag.index.run INFO read table from storage: create_base_documents.parquet
17:28:25,584 datashaper.workflow.workflow INFO executing verb rename
17:28:25,586 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_documents.parquet
