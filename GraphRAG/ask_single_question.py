import os
import asyncio
from dotenv import load_dotenv
load_dotenv(dotenv_path="./graphrag_index/.env")
import pandas as pd
import tiktoken
from graphrag.query.context_builder.entity_extraction import EntityVectorStoreKey
from graphrag.query.indexer_adapters import (
    read_indexer_entities,
    read_indexer_relationships,
    read_indexer_reports,
    read_indexer_text_units,
)
from graphrag.query.input.loaders.dfs import (
    store_entity_semantic_embeddings,
)
from graphrag.query.llm.oai.chat_openai import ChatOpenAI
from graphrag.query.llm.oai.embedding import OpenAIEmbedding
from graphrag.query.llm.oai.typing import OpenaiApiType
from graphrag.query.structured_search.local_search.mixed_context import (
    LocalSearchMixedContext,
)
from graphrag.query.structured_search.local_search.search import LocalSearch
from graphrag.vector_stores import MilvusVectorStore

# ==================== è¼‰å…¥ GraphRAG ç´¢å¼• ==================== #
print("ğŸ”„ è¼‰å…¥ GraphRAG ç´¢å¼•è³‡æ–™...")

index_root = os.path.join(os.getcwd(), 'graphrag_index')
output_dir = os.path.join(index_root, "output")
subdirs = [os.path.join(output_dir, d) for d in os.listdir(output_dir)]
latest_subdir = max(subdirs, key=os.path.getmtime)
INPUT_DIR = os.path.join(latest_subdir, "artifacts")

COMMUNITY_LEVEL = 2

# è¼‰å…¥å„é¡è³‡æ–™è¡¨
entity_df = pd.read_parquet(f"{INPUT_DIR}/create_final_nodes.parquet")
entity_embedding_df = pd.read_parquet(f"{INPUT_DIR}/create_final_entities.parquet")
relationship_df = pd.read_parquet(f"{INPUT_DIR}/create_final_relationships.parquet")
report_df = pd.read_parquet(f"{INPUT_DIR}/create_final_community_reports.parquet")
text_unit_df = pd.read_parquet(f"{INPUT_DIR}/create_final_text_units.parquet")

# è½‰æ›ç‚º GraphRAG æ ¼å¼
entities = read_indexer_entities(entity_df, entity_embedding_df, COMMUNITY_LEVEL)
relationships = read_indexer_relationships(relationship_df)
reports = read_indexer_reports(report_df, entity_df, COMMUNITY_LEVEL)
text_units = read_indexer_text_units(text_unit_df)

# è¨­ç½®å‘é‡è³‡æ–™åº«
description_embedding_store = MilvusVectorStore(
    collection_name="entity_description_embeddings",
)
description_embedding_store.connect(uri="./milvus.db")
entity_description_embeddings = store_entity_semantic_embeddings(
    entities=entities, vectorstore=description_embedding_store
)

print(f"âœ… è¼‰å…¥å®Œæˆ: {len(entities)} å€‹å¯¦é«”, {len(relationships)} å€‹é—œä¿‚")

# ==================== è¨­ç½® LLM å’Œæœå°‹å¼•æ“ ==================== #
api_key = os.environ["GRAPHRAG_API_KEY"]
llm_model = "gpt-4o-mini"  # ä½¿ç”¨è¼ƒä¾¿å®œçš„æ¨¡å‹
embedding_model = "text-embedding-3-small"

llm = ChatOpenAI(
    api_key=api_key,
    model=llm_model,
    api_type=OpenaiApiType.OpenAI,
    max_retries=20,
)

token_encoder = tiktoken.get_encoding("cl100k_base")

text_embedder = OpenAIEmbedding(
    api_key=api_key,
    api_base=None,
    api_type=OpenaiApiType.OpenAI,
    model=embedding_model,
    deployment_name=embedding_model,
    max_retries=20,
)

context_builder = LocalSearchMixedContext(
    community_reports=reports,
    text_units=text_units,
    entities=entities,
    relationships=relationships,
    covariates=None,
    entity_text_embeddings=description_embedding_store,
    embedding_vectorstore_key=EntityVectorStoreKey.ID,
    text_embedder=text_embedder,
    token_encoder=token_encoder,
)

local_context_params = {
    "text_unit_prop": 0.5,
    "community_prop": 0.1,
    "conversation_history_max_turns": 5,
    "conversation_history_user_turns_only": True,
    "top_k_mapped_entities": 10,
    "top_k_relationships": 10,
    "include_entity_rank": True,
    "include_relationship_weight": True,
    "include_community_rank": False,
    "return_candidate_context": False,
    "embedding_vectorstore_key": EntityVectorStoreKey.ID,
    "max_tokens": 12_000,
}

llm_params = {
    "max_tokens": 2000,
    "temperature": 0.0,
}

search_engine = LocalSearch(
    llm=llm,
    context_builder=context_builder,
    token_encoder=token_encoder,
    llm_params=llm_params,
    context_builder_params=local_context_params,
    response_type="multiple paragraphs",
)

# ==================== ä¸»æŸ¥è©¢å‡½æ•¸ ==================== #
async def ask_question(question: str):
    """
    å‘ GraphRAG æå•ä¸¦ç²å–ç­”æ¡ˆ
    
    Args:
        question: è¦è©¢å•çš„å•é¡Œ
    
    Returns:
        æœå°‹çµæœ
    """
    print(f"\nğŸ“ å•é¡Œ: {question}")
    print("ğŸ” æœå°‹ä¸­...")
    
    result = await search_engine.asearch(question)
    
    print(f"\nâœ… å›ç­”:\n{result.response}")
    print(f"\nâ±ï¸  æœå°‹æ™‚é–“: {result.completion_time:.2f} ç§’")
    print(f"ğŸ’¬ LLM å‘¼å«æ¬¡æ•¸: {result.llm_calls}")
    print(f"ğŸ“Š ä½¿ç”¨çš„ tokens: {result.prompt_tokens}")
    
    return result

async def ask_multiple_questions(questions: list):
    """
    æ‰¹é‡æå•ä¸¦ç²å–ç­”æ¡ˆ
    
    Args:
        questions: å•é¡Œåˆ—è¡¨
    
    Returns:
        çµæœåˆ—è¡¨
    """
    results = []
    total_time = 0
    total_llm_calls = 0
    total_tokens = 0
    
    for i, question in enumerate(questions, 1):
        print(f"\n{'=' * 80}")
        print(f"è™•ç†ç¬¬ {i}/{len(questions)} å€‹å•é¡Œ")
        print(f"{'=' * 80}")
        
        result = await ask_question(question)
        results.append({
            'question': question,
            'answer': result.response,
            'completion_time': result.completion_time,
            'llm_calls': result.llm_calls,
            'prompt_tokens': result.prompt_tokens
        })
        
        total_time += result.completion_time
        total_llm_calls += result.llm_calls
        total_tokens += result.prompt_tokens
    
    # é¡¯ç¤ºç¸½é«”çµ±è¨ˆ
    print(f"\n{'=' * 80}")
    print(f"ğŸ“Š ç¸½é«”çµ±è¨ˆ")
    print(f"{'=' * 80}")
    print(f"ç¸½å•é¡Œæ•¸: {len(questions)}")
    print(f"ç¸½æœå°‹æ™‚é–“: {total_time:.2f} ç§’")
    print(f"å¹³å‡æœå°‹æ™‚é–“: {total_time/len(questions):.2f} ç§’")
    print(f"ç¸½ LLM å‘¼å«æ¬¡æ•¸: {total_llm_calls}")
    print(f"ç¸½ä½¿ç”¨ tokens: {total_tokens}")
    
    return results

def read_questions_from_file(file_path: str):
    """
    å¾æ–‡ä»¶ä¸­è®€å–å•é¡Œåˆ—è¡¨
    
    Args:
        file_path: å•é¡Œæ–‡ä»¶è·¯å¾‘
    
    Returns:
        å•é¡Œåˆ—è¡¨
    """
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            questions = [line.strip() for line in f if line.strip()]
        print(f"âœ… æˆåŠŸå¾ {file_path} è®€å– {len(questions)} å€‹å•é¡Œ")
        return questions
    except FileNotFoundError:
        print(f"âŒ æ‰¾ä¸åˆ°æ–‡ä»¶: {file_path}")
        return []

# ==================== åŸ·è¡ŒæŸ¥è©¢ ==================== #
if __name__ == "__main__":
    # å¾ question_list.txt è®€å–å•é¡Œ
    question_file = "question_list.txt"
    questions = read_questions_from_file(question_file)
    
    if not questions:
        print("âŒ æ²’æœ‰æ‰¾åˆ°å•é¡Œï¼Œè«‹æª¢æŸ¥ question_list.txt æ–‡ä»¶")
    else:
        # åŸ·è¡Œæ‰¹é‡æŸ¥è©¢
        results = asyncio.run(ask_multiple_questions(questions))
        
        # å°‡çµæœä¿å­˜åˆ°æ–‡ä»¶
        output_file = "multiple_questions_result.md"
        with open(output_file, "w", encoding="utf-8") as f:
            f.write(f"GraphRAG æ‰¹é‡æŸ¥è©¢çµæœ\n")
            f.write(f"å•é¡Œç¸½æ•¸: {len(results)}\n")
            f.write(f"{'=' * 80}\n\n")
            
            for i, result in enumerate(results, 1):
                f.write(f"å•é¡Œ {i}: {result['question']}\n")
                f.write(f"{'-' * 80}\n")
                f.write(f"å›ç­”:\n{result['answer']}\n\n")
                f.write(f"æœå°‹æ™‚é–“: {result['completion_time']:.2f} ç§’\n")
                f.write(f"LLM å‘¼å«æ¬¡æ•¸: {result['llm_calls']}\n")
                f.write(f"ä½¿ç”¨ tokens: {result['prompt_tokens']}\n")
                f.write(f"{'=' * 80}\n\n")
            
            # æ·»åŠ ç¸½é«”çµ±è¨ˆ
            total_time = sum(r['completion_time'] for r in results)
            total_llm_calls = sum(r['llm_calls'] for r in results)
            total_tokens = sum(r['prompt_tokens'] for r in results)
            
            f.write(f"\n{'=' * 80}\n")
            f.write(f"ç¸½é«”çµ±è¨ˆ\n")
            f.write(f"{'=' * 80}\n")
            f.write(f"ç¸½å•é¡Œæ•¸: {len(results)}\n")
            f.write(f"ç¸½æœå°‹æ™‚é–“: {total_time:.2f} ç§’\n")
            f.write(f"å¹³å‡æœå°‹æ™‚é–“: {total_time/len(results):.2f} ç§’\n")
            f.write(f"ç¸½ LLM å‘¼å«æ¬¡æ•¸: {total_llm_calls}\n")
            f.write(f"ç¸½ä½¿ç”¨ tokens: {total_tokens}\n")
        
        print(f"\nğŸ’¾ æ‰€æœ‰çµæœå·²ä¿å­˜è‡³: {output_file}")
