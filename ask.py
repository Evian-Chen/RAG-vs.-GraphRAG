# ask.py â€” Multi-agent collaborative pipeline
# æ™ºèƒ½ä»£ç†å”ä½œæµç¨‹ï¼šDb agent -> Rewrite agent -> table_decide_agent -> table_process_agent -> data_analysis_agent
# æ”¯æ´ä»£ç†é–“å›é¥‹å¾ªç’°èˆ‡è³‡è¨Šå…±äº«æ©Ÿåˆ¶
# pip install pymongo[srv] sentence-transformers sqlalchemy psycopg2-binary python-dotenv openai

import os, json, re, math
from typing import List, Dict, Any, Tuple, Optional
from dataclasses import dataclass
from dotenv import load_dotenv
from sqlalchemy import create_engine, text, inspect
from sqlalchemy.engine import Engine
from pymongo import MongoClient
import certifi

# ---------- Agent Communication Protocol ----------
@dataclass
class AgentMessage:
    """ä»£ç†é–“é€šè¨Šè¨Šæ¯æ ¼å¼"""
    sender: str
    receiver: str
    message_type: str
    content: Dict[str, Any]
    timestamp: float = 0.0

@dataclass 
class PipelineContext:
    """Pipeline åŸ·è¡Œä¸Šä¸‹æ–‡ï¼Œç”¨æ–¼ä»£ç†é–“è³‡è¨Šå…±äº«"""
    user_query: str
    reference_context: str = ""
    db_overview: Dict[str, Any] = None
    rewritten_query: Dict[str, Any] = None
    table_plan: Dict[str, Any] = None
    processed_data: List[Dict[str, Any]] = None
    sql_query: str = ""
    analysis_result: str = ""
    agent_messages: List[AgentMessage] = None
    
    def __post_init__(self):
        if self.agent_messages is None:
            self.agent_messages = []

# ---------- Load env ----------
load_dotenv()
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY", "")
OPENAI_CHAT_MODEL = os.getenv("OPENAI_CHAT_MODEL", "gpt-4o-mini")

PG_URI  = os.getenv("PG_URI")
MONGO_URI = os.getenv("MONGO_URI")
MONGO_DB  = os.getenv("MONGO_DB", "ragdb")
MONGO_COL = os.getenv("MONGO_COL", "cards")
MONGO_VECTOR_INDEX = os.getenv("MONGO_VECTOR_INDEX", "cards_env")

if not PG_URI:
    raise RuntimeError("PG_URI is required (Neon connection string).")
if not OPENAI_API_KEY:
    raise RuntimeError("OPENAI_API_KEY is required.")
if not MONGO_URI:
    print("[warn] MONGO_URI not set; reference retrieval will be disabled.")

# ---------- OpenAI client ----------
from openai import OpenAI
oai = OpenAI(api_key=OPENAI_API_KEY)

def chat(messages, model=OPENAI_CHAT_MODEL, temperature=0.1, max_tokens=800):
    resp = oai.chat.completions.create(
        model=model,
        messages=messages,
        temperature=temperature,
        max_tokens=max_tokens,
    )
    return resp.choices[0].message.content.strip()

# ---------- Postgres engine ----------
pg_engine: Engine = create_engine(PG_URI, pool_pre_ping=True)

# ---------- Mongo (reference) ----------
def mongo_cards_collection():
    if not MONGO_URI:
        return None
    client = MongoClient(MONGO_URI, tls=True, tlsCAFile=certifi.where())
    return client[MONGO_DB][MONGO_COL]

def reference_search(query: str, k: int = 6) -> List[Dict[str, Any]]:
    """
    Vector search over Mongo Atlas Vector Search ($vectorSearch).
    Returns list of {title, type, text, meta, score}
    """
    col = mongo_cards_collection()
    if col is None:
        return []
    
    try:
        # ä½¿ç”¨ $vectorSearch å‰æï¼šä½ å·²å»ºç«‹ Vector ç´¢å¼•ï¼Œfield=embedding(384, cosine)
        from sentence_transformers import SentenceTransformer
        model = SentenceTransformer("sentence-transformers/all-MiniLM-L6-v2")
        qv = model.encode([query], normalize_embeddings=True)[0].tolist()

        pipeline = [
            {
                "$vectorSearch": {
                    "index": MONGO_VECTOR_INDEX,
                    "path": "embedding",
                    "queryVector": qv,
                    "numCandidates": max(100, k*20),
                    "limit": k
                }
            },
            {"$project": {
                "title": 1, "type": 1, "text": 1, "meta": 1,
                "score": {"$meta": "vectorSearchScore"}
            }}
        ]
        return list(col.aggregate(pipeline))
    except Exception as e:
        print(f"[warn] MongoDB vector search failed: {e}")
        return []

# ---------- Utility ----------
def clamp(n, lo, hi): return max(lo, min(hi, n))

def build_ref_context(cards: List[Dict[str, Any]], max_chars: int = 9000) -> str:
    """Turn retrieved cards into a compact context string."""
    out = []
    used = 0
    for d in cards:
        block = f"# {d.get('title','')}\n[type={d.get('type','')}] score={round(float(d.get('score',0)),4)}\n{d.get('text','')}\n"
        if used + len(block) > max_chars:
            break
        out.append(block); used += len(block)
    return "\n---\n".join(out)

def allowlist_from_pg(engine: Engine) -> Dict[str, List[str]]:
    """Fallback allowlist by introspecting PG public schema."""
    insp = inspect(engine)
    al: Dict[str, List[str]] = {}
    for t in insp.get_table_names(schema="public"):
        cols = [c["name"] for c in insp.get_columns(t, schema="public")]
        al[t] = cols
    return al

# ---------- Agent 1: Db Agent (è³‡æ–™åº«ä»£ç†) ----------
class DbAgent:
    """è² è²¬èˆ‡ PostgreSQL äº’å‹•ï¼Œæä¾›è³‡æ–™åº«çµæ§‹è³‡è¨Šèˆ‡è³‡æ–™æ“·å–"""
    
    def __init__(self, engine: Engine):
        self.engine = engine
        self.name = "DbAgent"
    
    def scan_schema(self, sample_rows: int = 5) -> Dict[str, Any]:
        """æƒæè³‡æ–™åº«çµæ§‹ä¸¦æä¾›æ¨£æœ¬è³‡æ–™"""
        insp = inspect(self.engine)
        info: Dict[str, Any] = {
            "tables": [],
            "scan_timestamp": math.floor(os.times().elapsed * 1000),
            "total_tables": 0
        }
        
        table_names = insp.get_table_names(schema="public")
        info["total_tables"] = len(table_names)
        
        for t in table_names:
            cols = insp.get_columns(t, schema="public")
            col_defs = [{"name": c["name"], "type": str(c["type"])} for c in cols]

            # å–å¾—æ¨£æœ¬è³‡æ–™
            with self.engine.begin() as conn:
                try:
                    rows = conn.execute(text(f'SELECT * FROM public."{t}" LIMIT :n'), {"n": sample_rows}).mappings().all()
                    rows = [dict(r) for r in rows]
                    row_count = conn.execute(text(f'SELECT COUNT(*) as cnt FROM public."{t}"')).scalar()
                except Exception as e:
                    rows = []
                    row_count = 0
                    
            info["tables"].append({
                "name": t, 
                "columns": col_defs, 
                "sample": rows,
                "total_rows": row_count
            })
        return info
    
    def execute_query(self, sql: str, max_rows: int = 20000) -> Tuple[List[Dict[str, Any]], str]:
        """å®‰å…¨åŸ·è¡Œ SQL æŸ¥è©¢"""
        if not re.match(r"^(with|select)\b", sql.strip(), re.IGNORECASE):
            return [], "Refused: not a SELECT/WITH statement."
        try:
            with self.engine.begin() as conn:
                res = conn.execute(text(sql))
                rows = res.mappings().fetchmany(size=max_rows)
                rows = [dict(r) for r in rows]
            return rows, ""
        except Exception as e:
            return [], f"{type(e).__name__}: {e}"
    
    def get_table_stats(self, table_name: str) -> Dict[str, Any]:
        """å–å¾—ç‰¹å®šè³‡æ–™è¡¨çš„çµ±è¨ˆè³‡è¨Š"""
        try:
            with self.engine.begin() as conn:
                # åŸºæœ¬çµ±è¨ˆ
                stats = {}
                stats["row_count"] = conn.execute(text(f'SELECT COUNT(*) FROM public."{table_name}"')).scalar()
                
                # æ¬„ä½çµ±è¨ˆ
                cols_info = conn.execute(text(f"""
                    SELECT column_name, data_type, is_nullable 
                    FROM information_schema.columns 
                    WHERE table_name = :tname AND table_schema = 'public'
                """), {"tname": table_name}).mappings().all()
                stats["columns"] = [dict(c) for c in cols_info]
                
                return stats
        except Exception as e:
            return {"error": str(e)}

# ---------- Agent 2: Rewrite Agent (æŸ¥è©¢æ”¹å¯«ä»£ç†) ----------
class RewriteAgent:
    """è² è²¬æ”¹å¯«å’Œå„ªåŒ–ä½¿ç”¨è€…æŸ¥è©¢ï¼Œçµåˆåƒè€ƒè³‡æ–™é€²è¡Œèªæ„è£œå¼·"""
    
    def __init__(self):
        self.name = "RewriteAgent"
        self.system_prompt = """You are a rewrite agent for data questions. 
Given a user query, database schema, and references, output a clarified task in VALID JSON format.

CRITICAL: Only use column names that ACTUALLY exist in the provided database schema.

Schema Validation Rules:
- ALWAYS check the provided db_schema for actual table and column names
- NEVER invent or assume column names that don't exist in the schema
- If a column name seems logical but doesn't exist, suggest alternatives from available columns
- LoginDate/PlayDate are integers YYYYMMDD (not date strings)
- Country uses ISO-2 codes like 'TW','US'

Return ONLY valid JSON (no markdown, no extra text):
{
  "goal": "Clear description of the analysis objective",
  "available_tables": ["List actual tables from schema"],
  "available_columns": {"table1": ["col1", "col2"], "table2": ["col3", "col4"]},
  "filters": {
    "LoginDate": {"start": 20241001, "end": 20241007},
    "Country": "TW"
  },
  "metrics": ["Only use columns that exist in schema"],
  "hints": ["Specific guidance based on actual schema"],
  "confidence": 0.8
}"""
    
    def rewrite_query(self, context: PipelineContext) -> Dict[str, Any]:
        """æ”¹å¯«ä½¿ç”¨è€…æŸ¥è©¢"""
        # æå–è©³ç´°çš„ schema ä¿¡æ¯
        schema_summary = {}
        if context.db_overview and "tables" in context.db_overview:
            for table in context.db_overview["tables"]:
                schema_summary[table["name"]] = [col["name"] for col in table["columns"]]
        
        prompt = f"""<reference>
{context.reference_context}
</reference>
<query>{context.user_query}</query>
<actual_db_schema>
Available tables and their columns:
{json.dumps(schema_summary, ensure_ascii=False, indent=2)}
</actual_db_schema>
<schema_details>
{json.dumps(context.db_overview.get('tables', [])[:5], ensure_ascii=False) if context.db_overview else '{}'}
</schema_details>
IMPORTANT: Only reference tables and columns that exist in the actual_db_schema above.
Follow the system rules and output JSON only."""
        
        out = chat([
            {"role":"system","content":self.system_prompt},
            {"role":"user","content":prompt}
        ], max_tokens=600)
        
        try:
            result = json.loads(out)
            # æ·»åŠ ä»£ç†è³‡è¨Š
            result["rewrite_agent"] = {
                "version": "1.0",
                "timestamp": math.floor(os.times().elapsed * 1000)
            }
            return result
        except Exception as e:
            return {
                "goal": context.user_query, 
                "available_tables": list(schema_summary.keys()),
                "available_columns": schema_summary,
                "filters": {}, 
                "metrics": [], 
                "hints": [out],
                "confidence": 0.3,
                "error": str(e)
            }
    
    def refine_query(self, context: PipelineContext, feedback: Dict[str, Any]) -> Dict[str, Any]:
        """æ ¹æ“šå›é¥‹ç²¾ç…‰æŸ¥è©¢"""
        refinement_prompt = f"""
Original query: {context.user_query}
Previous rewrite: {json.dumps(context.rewritten_query, ensure_ascii=False)}
Feedback from downstream agents: {json.dumps(feedback, ensure_ascii=False)}

Please refine the query based on the feedback. Output JSON only.
"""
        
        out = chat([
            {"role":"system","content":self.system_prompt + "\nYou are now refining based on feedback."},
            {"role":"user","content":refinement_prompt}
        ], max_tokens=400)
        
        try:
            result = json.loads(out)
            result["refined"] = True
            result["refinement_timestamp"] = math.floor(os.times().elapsed * 1000)
            return result
        except Exception:
            return context.rewritten_query  # å›å‚³åŸå§‹æ”¹å¯«çµæœ

# ---------- Agent 3: Table Decide Agent (è³‡æ–™è¡¨æ±ºç­–ä»£ç†) ----------
class TableDecideAgent:
    """è² è²¬æ ¹æ“šæ”¹å¯«å¾Œçš„æŸ¥è©¢æ±ºå®šä½¿ç”¨å“ªäº›è³‡æ–™è¡¨èˆ‡æ¬„ä½"""
    
    def __init__(self):
        self.name = "TableDecideAgent"
        self.system_prompt = """You are a table selection agent with STRICT schema validation.
You receive (1) rewritten intent JSON and (2) a DB schema overview.

CRITICAL SCHEMA VALIDATION:
- ONLY use tables and columns that ACTUALLY exist in the provided db_overview
- NEVER assume or invent column names
- If a logical column doesn't exist, find the closest match or suggest alternatives
- Validate ALL column references against the actual schema

Rules:
- Check db_overview["tables"] for exact table names and column lists
- LoginDate/PlayDate are integers YYYYMMDD if they exist
- Country is two-letter code if it exists
- If a required column is missing, mark it in alternatives

Return STRICT JSON with schema validation:
{
 "tables": [{"name":"ActualTableName","columns":["ActualCol1","ActualCol2"], "priority": 1}],
 "joins": [{"left":"Table1.ActualCol","right":"Table2.ActualCol","type":"inner"}],
 "filters": {"ActualColumn":"value", "date_col":"ActualDateColumn", "start":20241001, "end":20241031},
 "limit": 100000,
 "confidence": 0.9,
 "reason": "Explain which actual tables/columns were found",
 "schema_issues": ["List any missing or assumed columns"],
 "alternatives": ["Suggest actual columns if ideal ones don't exist"]
}"""
    
    def decide_tables(self, context: PipelineContext) -> Dict[str, Any]:
        """æ±ºå®šéœ€è¦ä½¿ç”¨çš„è³‡æ–™è¡¨"""
        prompt = f"""<intent_json>
{json.dumps(context.rewritten_query, ensure_ascii=False)}
</intent_json>
<db_overview>
{json.dumps(context.db_overview, ensure_ascii=False)}
</db_overview>
Output the strict JSON schema specified by the system."""
        
        out = chat([
            {"role":"system","content":self.system_prompt},
            {"role":"user","content":prompt}
        ], max_tokens=900)
        
        # æ¸…ç†å¯èƒ½çš„ markdown åŒ…è£
        clean_out = out.strip()
        if clean_out.startswith("```json"):
            clean_out = clean_out[7:]
        if clean_out.startswith("```"):
            clean_out = clean_out[3:]
        if clean_out.endswith("```"):
            clean_out = clean_out[:-3]
        clean_out = clean_out.strip()
        
        try:
            plan = json.loads(clean_out)
            # æ·»åŠ æ±ºç­–å…ƒè³‡è¨Š
            plan["decision_agent"] = {
                "version": "1.0", 
                "timestamp": math.floor(os.times().elapsed * 1000)
            }
            return plan
        except Exception as e:
            return {
                "tables": [], 
                "joins": [], 
                "filters": {}, 
                "limit": 100000, 
                "confidence": 0.1,
                "reason": str(e),
                "error": out
            }
    
    def validate_plan(self, plan: Dict[str, Any], db_overview: Dict[str, Any]) -> Dict[str, Any]:
        """é©—è­‰è³‡æ–™è¡¨é¸æ“‡è¨ˆç•«çš„å¯è¡Œæ€§"""
        validation_result = {
            "valid": True,
            "issues": [],
            "suggestions": []
        }
        
        available_tables = {t["name"] for t in db_overview.get("tables", [])}
        
        # æª¢æŸ¥è¡¨æ ¼æ˜¯å¦å­˜åœ¨
        for table_info in plan.get("tables", []):
            table_name = table_info.get("name")
            if table_name not in available_tables:
                validation_result["valid"] = False
                validation_result["issues"].append(f"Table '{table_name}' not found in database")
        
        # æª¢æŸ¥ JOIN çš„å¯è¡Œæ€§
        for join_info in plan.get("joins", []):
            left_parts = join_info.get("left", "").split(".")
            right_parts = join_info.get("right", "").split(".")
            
            if len(left_parts) != 2 or len(right_parts) != 2:
                validation_result["issues"].append(f"Invalid join format: {join_info}")
        
        return validation_result

# ---------- Agent 4: Table Process Agent (è³‡æ–™è¡¨è™•ç†ä»£ç†) ----------
class TableProcessAgent:
    """è² è²¬è™•ç†é¸å®šçš„è³‡æ–™è¡¨ï¼ŒåŒ…æ‹¬SQLç”Ÿæˆã€åŸ·è¡Œèˆ‡è³‡æ–™è™•ç†"""
    
    def __init__(self, db_agent: DbAgent):
        self.name = "TableProcessAgent"
        self.db_agent = db_agent
        self.system_prompt = """You are a SQL composer and data processor. 
Generate ONE single SELECT (or WITH ... SELECT) for PostgreSQL.
CONSTRAINTS:
- Only read. NO DDL/DML. NO semicolon in the middle.
- If date range present and date column is integer YYYYMMDD (e.g., LoginDate/PlayDate), use numeric BETWEEN.
- Country must compare to ISO-2 string exactly (e.g., Country='TW').
- Add LIMIT at the end (use provided limit or 1000).
- Prefer simple aggregates for trend (GROUP BY day integer).
- Add meaningful column aliases for better readability.
- IMPORTANT: Always wrap table names and column names with double quotes for PostgreSQL (e.g., FROM public."TableName", SELECT "ColumnName").
Return the SQL only.
"""
    
    def generate_sql(self, context: PipelineContext, error_feedback: str = "") -> str:
        """æ ¹æ“šè¨ˆç•«ç”ŸæˆSQLèªå¥ï¼Œæ”¯æ´éŒ¯èª¤å›é¥‹ä¿®æ­£"""
        base_prompt = f"""<plan>
{json.dumps(context.table_plan, ensure_ascii=False)}
</plan>
<original_query>
{context.user_query}
</original_query>
<db_schema_detail>
{json.dumps([{"name": t["name"], "columns": [c["name"] for c in t["columns"]]} for t in context.db_overview.get("tables", [])], ensure_ascii=False)}
</db_schema_detail>"""

        if error_feedback:
            prompt = f"""{base_prompt}
<error_feedback>
Previous SQL failed with error: {error_feedback}
Please fix the SQL by:
1. Using only columns that exist in the schema
2. Correcting table names and column references
3. Fixing syntax issues
</error_feedback>
Generate corrected SQL now."""
        else:
            prompt = f"""{base_prompt}
Generate the SQL now."""
        
        sql = chat([
            {"role":"system","content":self.system_prompt},
            {"role":"user","content":prompt}
        ], max_tokens=600)
        
        # æ¸…ç† markdown æ ¼å¼
        sql = sql.strip()
        if sql.startswith("```sql"):
            sql = sql[6:]
        if sql.startswith("```"):
            sql = sql[3:]
        if sql.endswith("```"):
            sql = sql[:-3]
        sql = sql.strip().rstrip(";")
        
        # å®‰å…¨æ€§æª¢æŸ¥ï¼šåªå…è¨± SELECT/WITH
        if not re.match(r"^(with|select)\b", sql, re.IGNORECASE):
            sql = "-- invalid_non_select\n" + sql
        
        # ç¢ºä¿æœ‰ LIMIT
        if re.search(r"\blimit\b", sql, re.IGNORECASE) is None:
            limit = context.table_plan.get("limit", 1000) if context.table_plan else 1000
            sql += f"\nLIMIT {limit}"
        
        return sql
    
    def execute_and_process(self, context: PipelineContext) -> Tuple[List[Dict[str, Any]], str]:
        """åŸ·è¡ŒSQLä¸¦è™•ç†çµæœè³‡æ–™"""
        sql = context.sql_query
        rows, error = self.db_agent.execute_query(sql)
        
        if error:
            return [], error
        
        # è³‡æ–™å¾Œè™•ç†
        processed_rows = self._post_process_data(rows, context)
        return processed_rows, ""
    
    def _post_process_data(self, rows: List[Dict[str, Any]], context: PipelineContext) -> List[Dict[str, Any]]:
        """å°æŸ¥è©¢çµæœé€²è¡Œå¾Œè™•ç†"""
        if not rows:
            return rows
        
        processed = []
        for row in rows:
            processed_row = {}
            for key, value in row.items():
                # è™•ç† Decimal é¡å‹
                from decimal import Decimal
                if isinstance(value, Decimal):
                    value = float(value)
                
                # æ—¥æœŸæ ¼å¼è½‰æ›
                if key.lower().find('date') != -1 and isinstance(value, int) and len(str(value)) == 8:
                    processed_row[key] = value
                    processed_row[f"{key}_formatted"] = f"{str(value)[:4]}-{str(value)[4:6]}-{str(value)[6:8]}"
                else:
                    processed_row[key] = value
            processed.append(processed_row)
        
        return processed
    
    def validate_sql(self, sql: str) -> Dict[str, Any]:
        """é©—è­‰SQLèªå¥çš„å®‰å…¨æ€§å’Œæ­£ç¢ºæ€§"""
        validation = {
            "safe": True,
            "issues": [],
            "warnings": []
        }
        
        # åŸºæœ¬å®‰å…¨æª¢æŸ¥
        dangerous_keywords = ['drop', 'delete', 'insert', 'update', 'alter', 'truncate', 'create']
        sql_lower = sql.lower()
        
        for keyword in dangerous_keywords:
            if keyword in sql_lower:
                validation["safe"] = False
                validation["issues"].append(f"Dangerous keyword detected: {keyword}")
        
        # æª¢æŸ¥æ˜¯å¦ç‚º SELECT èªå¥
        if not re.match(r"^(with|select)\b", sql.strip(), re.IGNORECASE):
            validation["safe"] = False
            validation["issues"].append("Not a SELECT or WITH statement")
        
        return validation

# ---------- Agent 5: Data Analysis Agent (è³‡æ–™åˆ†æä»£ç†) ----------
class DataAnalysisAgent:
    """è² è²¬å°è™•ç†å¾Œçš„è³‡æ–™é€²è¡Œåˆ†æï¼Œç”¢ç”Ÿæœ€çµ‚å ±å‘Š"""
    
    def __init__(self):
        self.name = "DataAnalysisAgent"
        self.system_prompt = """You are a data analysis agent. 
Given the user question, the final SQL, and query rows (may be truncated), produce:
- key findings (bulleted in traditional Chinese)
- one-sentence conclusion (in traditional Chinese)
- if row count is 0, propose next steps (e.g., date col mismatch, value mismatch)
- suggest follow-up questions if relevant
Keep it concise and actionable. Respond in Traditional Chinese.
"""
    
    def analyze_data(self, context: PipelineContext) -> str:
        """åˆ†æè³‡æ–™ä¸¦ç”¢ç”Ÿå ±å‘Š"""
        sample = context.processed_data[:50] if context.processed_data else []
        
        payload = {
            "question": context.user_query,
            "sql": context.sql_query,
            "preview_rows": sample,
            "preview_count": len(sample),
            "total_rows": len(context.processed_data) if context.processed_data else 0,
            "agent_messages": [
                {"agent": msg.sender, "type": msg.message_type, "summary": str(msg.content)[:100]}
                for msg in context.agent_messages[-3:] if context.agent_messages
            ]
        }
        
        out = chat([
            {"role":"system","content":self.system_prompt},
            {"role":"user","content":json.dumps(payload, ensure_ascii=False)}
        ], max_tokens=600)
        
        return out
    
    def generate_feedback_to_agents(self, context: PipelineContext) -> Dict[str, Any]:
        """ç”Ÿæˆçµ¦å…¶ä»–ä»£ç†çš„å›é¥‹è¨Šæ¯"""
        feedback = {
            "data_quality": "good" if context.processed_data and len(context.processed_data) > 0 else "poor",
            "row_count": len(context.processed_data) if context.processed_data else 0,
            "suggestions": []
        }
        
        # æ ¹æ“šçµæœå“è³ªæä¾›å»ºè­°
        if feedback["row_count"] == 0:
            feedback["suggestions"].append("Consider adjusting filters or date ranges")
            feedback["suggestions"].append("Verify table names and column mappings")
        elif feedback["row_count"] < 10:
            feedback["suggestions"].append("Results may be too limited, consider broader criteria")
        
        return feedback

# ---------- Agent Coordinator (ä»£ç†å”èª¿å™¨) ----------
class AgentCoordinator:
    """å”èª¿æ‰€æœ‰ä»£ç†çš„åŸ·è¡Œèˆ‡æºé€š"""
    
    def __init__(self, engine: Engine):
        self.db_agent = DbAgent(engine)
        self.rewrite_agent = RewriteAgent()
        self.table_decide_agent = TableDecideAgent()
        self.table_process_agent = TableProcessAgent(self.db_agent)
        self.data_analysis_agent = DataAnalysisAgent()
        
    def execute_pipeline(self, user_query: str, ref_context: str = "") -> PipelineContext:
        """åŸ·è¡Œå®Œæ•´çš„å¤šä»£ç†æµç¨‹"""
        # åˆå§‹åŒ– context
        context = PipelineContext(
            user_query=user_query,
            reference_context=ref_context
        )
        
        # Step 1: è³‡æ–™åº«ä»£ç†æƒæçµæ§‹
        context.db_overview = self.db_agent.scan_schema(sample_rows=3)
        self._add_message(context, "DbAgent", "System", "schema_scan", 
                         {"tables_found": context.db_overview["total_tables"]})
        
        # Step 2: æ”¹å¯«ä»£ç†è™•ç†æŸ¥è©¢
        context.rewritten_query = self.rewrite_agent.rewrite_query(context)
        self._add_message(context, "RewriteAgent", "TableDecideAgent", "query_rewritten",
                         {"confidence": context.rewritten_query.get("confidence", 0.5)})
        
        # Step 3: è³‡æ–™è¡¨æ±ºç­–ä»£ç†
        context.table_plan = self.table_decide_agent.decide_tables(context)
        validation = self.table_decide_agent.validate_plan(context.table_plan, context.db_overview)
        
        # å¦‚æœè¨ˆç•«æœ‰å•é¡Œï¼Œå›é¥‹çµ¦æ”¹å¯«ä»£ç†
        if not validation["valid"]:
            feedback = {"issues": validation["issues"], "db_available": list(context.db_overview.keys())}
            refined_query = self.rewrite_agent.refine_query(context, feedback)
            context.rewritten_query = refined_query
            context.table_plan = self.table_decide_agent.decide_tables(context)
        
        self._add_message(context, "TableDecideAgent", "TableProcessAgent", "plan_decided",
                         {"tables_count": len(context.table_plan.get("tables", []))})
        
        # Step 4: è³‡æ–™è¡¨è™•ç†ä»£ç† (æ”¯æ´é‡è©¦æ©Ÿåˆ¶)
        max_retries = 2
        retry_count = 0
        
        while retry_count <= max_retries:
            if retry_count == 0:
                context.sql_query = self.table_process_agent.generate_sql(context)
            else:
                # é‡è©¦æ™‚æä¾›éŒ¯èª¤å›é¥‹
                context.sql_query = self.table_process_agent.generate_sql(context, error)
                self._add_message(context, "TableProcessAgent", "System", "sql_retry",
                                 {"retry_attempt": retry_count, "previous_error": error})
            
            context.processed_data, error = self.table_process_agent.execute_and_process(context)
            
            if not error:
                # æˆåŠŸåŸ·è¡Œ
                self._add_message(context, "TableProcessAgent", "DataAnalysisAgent", "data_ready",
                                 {"rows_processed": len(context.processed_data), "retries_used": retry_count})
                break
            else:
                retry_count += 1
                if retry_count <= max_retries:
                    self._add_message(context, "TableProcessAgent", "TableDecideAgent", "execution_error_retry",
                                     {"error": error, "retry_count": retry_count})
                    
                    # å¦‚æœæ˜¯ schema ç›¸é—œéŒ¯èª¤ï¼Œé‡æ–°ç”Ÿæˆ table plan
                    if "does not exist" in error or "UndefinedTable" in error or "UndefinedColumn" in error:
                        error_feedback = {
                            "sql_error": error,
                            "available_tables": [t["name"] for t in context.db_overview.get("tables", [])],
                            "suggestion": "Use only tables and columns that exist in schema"
                        }
                        context.table_plan = self.table_decide_agent.decide_tables(context)
                else:
                    # æœ€çµ‚å¤±æ•—
                    self._add_message(context, "TableProcessAgent", "DataAnalysisAgent", "execution_failed",
                                     {"final_error": error, "total_retries": retry_count - 1})
        
        # Step 5: è³‡æ–™åˆ†æä»£ç†
        context.analysis_result = self.data_analysis_agent.analyze_data(context)
        
        # ç”Ÿæˆå›é¥‹çµ¦å…¶ä»–ä»£ç†
        feedback = self.data_analysis_agent.generate_feedback_to_agents(context)
        self._add_message(context, "DataAnalysisAgent", "System", "analysis_complete", feedback)
        
        return context
    
    def _add_message(self, context: PipelineContext, sender: str, receiver: str, 
                    msg_type: str, content: Dict[str, Any]):
        """æ·»åŠ ä»£ç†é–“é€šè¨Šè¨Šæ¯"""
        message = AgentMessage(
            sender=sender,
            receiver=receiver, 
            message_type=msg_type,
            content=content,
            timestamp=math.floor(os.times().elapsed * 1000)
        )
        context.agent_messages.append(message)

# ---------- åˆå§‹åŒ–å…¨åŸŸä»£ç†å”èª¿å™¨ ----------
coordinator = AgentCoordinator(pg_engine)

# ---------- Top-level ask() function ----------
def ask(user_query: str) -> str:
    """ä¸»è¦çš„æŸ¥è©¢å…¥å£é»ï¼Œä½¿ç”¨å¤šä»£ç†å”ä½œæµç¨‹"""
    
    # 0) Reference retrieval (Mongo Vector Search)
    ref_cards = reference_search(user_query, k=6)
    ref_context = build_ref_context(ref_cards, max_chars=9000)
    
    # 1) åŸ·è¡Œå¤šä»£ç†å”ä½œæµç¨‹
    context = coordinator.execute_pipeline(user_query, ref_context)
    
    # 2) ç”Ÿæˆè©³ç´°è¼¸å‡ºå ±å‘Š
    lines = []
    lines.append("ğŸ¤– == Multi-Agent Pipeline Execution Report == ğŸ¤–\n")
    
    # ä»£ç†é€šè¨Šè¨˜éŒ„
    if context.agent_messages:
        lines.append("ğŸ“¡ [Agent Communications]")
        for msg in context.agent_messages:
            lines.append(f"  {msg.sender} â†’ {msg.receiver}: {msg.message_type}")
            if msg.content:
                content_summary = str(msg.content)[:100] + "..." if len(str(msg.content)) > 100 else str(msg.content)
                lines.append(f"    Content: {content_summary}")
        lines.append("")
    
    # æŸ¥è©¢æ”¹å¯«çµæœ
    lines.append("âœï¸  [Query Rewrite Result]")
    if context.rewritten_query:
        lines.append(json.dumps(context.rewritten_query, ensure_ascii=False, indent=2))
    lines.append("")
    
    # è³‡æ–™è¡¨é¸æ“‡è¨ˆç•«
    lines.append("ğŸ“‹ [Table Selection Plan]")
    if context.table_plan:
        lines.append(json.dumps(context.table_plan, ensure_ascii=False, indent=2))
    lines.append("")
    
    # SQL æŸ¥è©¢èªå¥
    lines.append("ğŸ’¾ [Generated SQL]")
    lines.append(context.sql_query if context.sql_query else "No SQL generated")
    lines.append("")
    
    # è³‡æ–™çµæœé è¦½
    lines.append(f"ğŸ“Š [Data Results] {min(len(context.processed_data), 10) if context.processed_data else 0} of {len(context.processed_data) if context.processed_data else 0} rows")
    if context.processed_data:
        for i, row in enumerate(context.processed_data[:10]):
            lines.append(f"  {i+1}: {str(row)}")
    else:
        lines.append("  No data returned")
    lines.append("")
    
    # è³‡æ–™åˆ†æçµæœ
    lines.append("ğŸ” [Analysis Report]")
    lines.append(context.analysis_result if context.analysis_result else "No analysis available")
    lines.append("")
    
    # åƒè€ƒè³‡æ–™å‘½ä¸­
    if ref_cards:
        lines.append("ğŸ“š [Reference Hits]")
        titles = [d.get("title","") for d in ref_cards]
        scores = [round(float(d.get("score",0)),4) for d in ref_cards]
        for t, s in zip(titles, scores):
            lines.append(f"  - {s} Â· {t}")
        lines.append("")
    
    # åŸ·è¡Œæ‘˜è¦
    lines.append("ğŸ“ˆ [Execution Summary]")
    lines.append(f"  â€¢ Total agents involved: 5")
    lines.append(f"  â€¢ Messages exchanged: {len(context.agent_messages)}")
    lines.append(f"  â€¢ Tables analyzed: {context.db_overview['total_tables'] if context.db_overview else 0}")
    lines.append(f"  â€¢ Rows processed: {len(context.processed_data) if context.processed_data else 0}")
    
    return "\n".join(lines)

if __name__ == "__main__":
    # ğŸ¯ 10å€‹æ ¸å¿ƒåˆ†æå•é¡Œ - åŸºæ–¼å¯¦éš› schema çµæ§‹å„ªåŒ–ï¼Œä½¿ç”¨å­˜åœ¨çš„æ¬„ä½
    analysis_questions = [
        # ç©å®¶æ´»èºåº¦åˆ†æ (ä½¿ç”¨ SessionActive è¡¨çš„å¯¦éš›æ¬„ä½)
        "ä½¿ç”¨ SessionActive è¡¨ä¸­çš„ LoginDateã€VipLVã€Country æ¬„ä½ï¼Œåˆ†æ 2024-10 æœˆå°ç£åœ°å€(Country='TW')å„ VIP ç­‰ç´šçš„ç™»å…¥æ´»èºåº¦åˆ†ä½ˆï¼Œè¨ˆç®—æ¯å€‹ VIP ç­‰ç´šçš„ç¸½ç™»å…¥æ¬¡æ•¸å’Œå¹³å‡æ¯æ—¥æ´»èºåº¦ï¼Œæ‰¾å‡ºæœ€æ´»èºçš„ VIP ç¾¤é«”ä¸¦åˆ†æå…¶ç™»å…¥è¶¨å‹¢",
        
        # è¨­å‚™èˆ‡å¹³å°åˆ†æ (ä½¿ç”¨å¤šè¡¨å¯¦éš›æ¬„ä½)
        "æ•´åˆ SessionActive å’Œ SessionLength è¡¨ä¸­çš„ SysTypeã€Countryã€VipLVã€Channel æ¬„ä½ï¼Œåˆ†æä¸åŒç³»çµ±é¡å‹çš„ç©å®¶åˆ†ä½ˆç‰¹å¾µã€å¹³å°æ´»èºç¨‹åº¦å°æ¯”ã€å„ç³»çµ±çš„ VIP è½‰åŒ–ç‡ï¼Œä»¥åŠè¨­å‚™é¡å‹å°éŠæˆ²è¡Œç‚ºçš„å½±éŸ¿",
        
        # æ¶ˆè²»è½‰åŒ–åˆ†æ (ä½¿ç”¨ _p_GameConsume è¡¨çš„å¯¦éš›æ¬„ä½)
        "é‹ç”¨ _p_GameConsume è¡¨ä¸­çš„ VipLVã€VipLVBeforeã€VipLVAfterã€BuyNumberActualã€CreateDate æ¬„ä½ï¼Œåˆ†æç©å®¶ VIP ç­‰ç´šå‡ç´šè»Œè·¡ï¼Œè¨ˆç®—å¾ä½ç­‰ç´šåˆ°é«˜ç­‰ç´šçš„æ¶ˆè²»è½‰åŒ–ç‡ã€å‡ç´šæ‰€éœ€çš„å¹³å‡æ¶ˆè²»é¡ï¼Œä»¥åŠ VIP å‡ç´šå°å¾ŒçºŒæ¶ˆè²»è¡Œç‚ºçš„å½±éŸ¿",
        
        # ç©å®¶ç•™å­˜åˆ†æ (ä½¿ç”¨ SessionActive è¡¨çš„å¯¦éš›æ¬„ä½)
        "åˆ©ç”¨ SessionActive è¡¨ä¸­çš„ UDIDã€LoginDateã€Countryã€VipLVã€SessionID æ¬„ä½ï¼Œè¿½è¹¤ 2024-10-01 åˆ° 2024-10-07 æœŸé–“æ–°ç™»å…¥ç©å®¶çš„ 7 æ—¥ç•™å­˜æƒ…æ³ï¼Œåˆ†æä¸åŒ VIP ç­‰ç´šæ–°ç©å®¶çš„ç•™å­˜ç‡å·®ç•°å’Œæµå¤±è¦å¾‹",
        
        # æ•´åˆæ¥­å‹™åˆ†æ (ä½¿ç”¨è·¨è¡¨å¯¦éš›æ¬„ä½)
        "ç¶œåˆé‹ç”¨ SessionActive è¡¨çš„ UDIDã€LoginDateã€VipLV å’Œ _p_GameConsume è¡¨çš„ UDIDã€BuyNumberActualã€CreateDate æ¬„ä½ï¼Œé€šé UDID é—œè¯åˆ†æï¼Œå»ºç«‹ç©å®¶åƒ¹å€¼è©•åˆ†æ¨¡å‹ï¼Œçµåˆç™»å…¥é »ç‡ã€æ¶ˆè²»é‡‘é¡ã€VIP ç­‰ç´šè®ŠåŒ–ï¼Œè­˜åˆ¥æœ€æœ‰åƒ¹å€¼çš„ç©å®¶ç¾¤é«”ç‰¹å¾µå’Œè¡Œç‚ºæ¨¡å¼",
        
        "åˆ†æ 2024 å¹´ 10 æœˆå„éŠæˆ²çš„å¹³å‡éŠæˆ²æ™‚é•·åˆ†ä½ˆï¼Œæ ¹æ“šå¯¦éš›çš„éŠæˆ²è¨˜éŒ„è³‡æ–™ï¼ˆåŒ…å«éŠæˆ²ä»£ç¢¼ã€æœƒè©±æ™‚é•·ã€æ—¥æœŸã€åœ°å€ç­‰çœŸå¯¦æ¬„ä½ï¼‰ï¼Œè¨ˆç®—æ¯æ¬¾éŠæˆ²çš„ç¸½éŠæˆ²æ™‚æ•¸ã€å¹³å‡æœƒè©±æ™‚é•·èˆ‡ç©å®¶åƒèˆ‡åº¦ï¼Œæ‰¾å‡ºæœ€å—æ­¡è¿ä¸”é»æ€§æœ€é«˜çš„éŠæˆ²é¡å‹ã€‚",
        
        "ä»¥å¯¦éš›çš„ç©å®¶æ¶ˆè²»ç´€éŒ„è³‡æ–™ç‚ºåŸºç¤ï¼ˆåŒ…å« VIP ç­‰ç´šã€è³¼è²·æ¬¡æ•¸ã€é‡‘é¡ã€æ—¥æœŸã€åœ°å€ã€éŠ·å”®é¡å‹ç­‰çœŸå¯¦æ¬„ä½ï¼‰ï¼Œæ·±å…¥åˆ†æä¸åŒ VIP ç­‰ç´šç©å®¶çš„æ¶ˆè²»æ¨¡å¼ï¼Œè§€å¯Ÿç¸½æ¶ˆè²»é‡‘é¡ã€è³¼è²·é »ç‡ã€åå¥½çš„éŠ·å”®é¡å‹èˆ‡æ™‚é–“åˆ†ä½ˆï¼Œå»ºç«‹é«˜åƒ¹å€¼å®¢ç¾¤çš„è¡Œç‚ºè¼ªå»“ã€‚",
        
        "çµåˆç©å®¶çš„ç™»å…¥èˆ‡æ¶ˆè²»ç­‰çœŸå¯¦è³‡æ–™ï¼ˆå«åœ°å€ã€VIP ç­‰ç´šã€æ¸ é“ã€å€åŸŸç­‰æ¬„ä½ï¼‰ï¼Œæ¯”è¼ƒå°ç£ï¼ˆTWï¼‰èˆ‡ç¾åœ‹ï¼ˆUSï¼‰ç©å®¶åœ¨ VIP ç­‰ç´šåˆ†ä½ˆã€æ¶ˆè²»ç¿’æ…£ã€æ¸ é“åå¥½ä¸Šçš„å·®ç•°ï¼Œè­˜åˆ¥åœ°å€æ€§éŠæˆ²è¡Œç‚ºç‰¹å¾µèˆ‡æ–‡åŒ–å·®ç•°ã€‚",
        
        "æ ¹æ“šå¯¦éš›çš„éŠæˆ²æ´»å‹•ç´€éŒ„ï¼ˆå«éŠæˆ²ä»£ç¢¼ã€æ—¥æœŸã€åœ°å€ã€ç³»çµ±é¡å‹ã€VIP ç­‰ç´šç­‰çœŸå¯¦æ¬„ä½ï¼‰ï¼Œæ‰¾å‡º 2024 å¹´ 10 æœˆæœ€å—æ­¡è¿çš„å‰ 10 æ¬¾éŠæˆ²ï¼Œä¸¦åˆ†æä¸åŒç³»çµ±å¹³å°èˆ‡ VIP ç­‰ç´šå°éŠæˆ²åå¥½çš„å½±éŸ¿ã€‚",
        
        "åˆ©ç”¨çœŸå¯¦çš„ç©å®¶ç™»å…¥èˆ‡æ´»å‹•æ™‚é–“è³‡æ–™ï¼ˆåŒ…å«ç™»å…¥æ—¥æœŸã€æ™‚æ®µã€åœ°å€ã€VIP ç­‰ç´šç­‰æ¬„ä½ï¼‰ï¼Œè§€å¯Ÿ 2024 å¹´ 10 æœˆæ¯æ—¥ç©å®¶æ´»èºåº¦çš„è®ŠåŒ–è¶¨å‹¢ï¼Œæ¯”è¼ƒå·¥ä½œæ—¥èˆ‡é€±æœ«çš„ç™»å…¥æ¨¡å¼å·®ç•°ï¼Œä¸¦åˆ†æä¸åŒ VIP ç­‰ç´šç©å®¶çš„æ™‚é–“åå¥½èˆ‡æ´»èºæ³¢å‹•ã€‚"
    ]
    
    import sys
    if len(sys.argv) > 1:
        try:
            # ä½¿ç”¨å‘½ä»¤åˆ—åƒæ•¸é¸æ“‡å•é¡Œç·¨è™Ÿ (1-20)
            question_id = int(sys.argv[1]) - 1
            if 0 <= question_id < len(analysis_questions):
                q = analysis_questions[question_id]
                print(f"ğŸ¯ åŸ·è¡Œå•é¡Œ #{question_id + 1}: {q}\n")
            else:
                print(f"âŒ å•é¡Œç·¨è™Ÿæ‡‰è©²åœ¨ 1-{len(analysis_questions)} ä¹‹é–“")
                sys.exit(1)
        except ValueError:
            # å¦‚æœä¸æ˜¯æ•¸å­—ï¼Œå°±ç•¶ä½œè‡ªå®šç¾©å•é¡Œ
            q = sys.argv[1]
            print(f"ğŸ¯ åŸ·è¡Œè‡ªå®šç¾©å•é¡Œ: {q}\n")
    else:
        # é»˜èªåŸ·è¡Œç¬¬ä¸€å€‹å•é¡Œ
        q = analysis_questions[0]
        print(f"ğŸ¯ åŸ·è¡Œé è¨­å•é¡Œ: {q}\n")
        print("ğŸ’¡ æç¤ºï¼šä½¿ç”¨ 'python ask.py <å•é¡Œç·¨è™Ÿ1-10>' ä¾†åŸ·è¡Œå…¶ä»–å•é¡Œ")
        print("ğŸ’¡ æç¤ºï¼šä½¿ç”¨ 'python ask.py \"æ‚¨çš„å•é¡Œ\"' ä¾†åŸ·è¡Œè‡ªå®šç¾©å•é¡Œ\n")
    
    print(ask(q))
